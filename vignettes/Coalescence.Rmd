---
title: "Coalescent Simulation and Application"
author: "Bruce J Cochrane"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Coalescent Simulation and Application}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---
```{r}
library(TeachingPopGen)
```

### Introduction

Inference based on inferred gene genealogies is one of the most important aspects of modern population genetics, however it is one that is among the most difficult for students to appreciate.  The theoretical basis, originating with Kingman (1982) has been thoroughly summarized (e. g. Hein, 2005; Wakeley, 2008); in essence, it is based on the idea that, given a sample of alleles, if one looks backward in time, eventually they “coalesce” on a most recent common ancestor (MRCA).    The dynamics of that process are determined by population size (N) and mutation rate (µ); the question we wish to address is, given an observed set of DNA sequences, can their pattern of diversity be explained by a neutral coalescent process?

What follows is based largely on use of the canonical coalescent simulation package **ms** (Hudson, 1991).  [The documentation for that program](https://webshare.uchicago.edu/users/rhudson1/Public/ms.folder/msdoc.pdf?ticket=t_DunQ7c99) is an essential first step.  The program is written in C and can be readily compiled and run from the command line under either Linux or Mac (and with some pain under Windows). Fortunately, however, it has been incorporated into the R package **phyclust**, which is loaded as a dependency by TeachingPopGen.  In addition, Hudson's basic distribution of ms includes an R script that converts the raw output of ms into a list, amenable to further analysis in R.

### Simulations

The goal of coaelescent simulation is to find a set of parameters, involving mutation rate and population size (&theta;), population history, recombination rates, etc., that best explain the distribution of nucleotide sequence variation in a real data set of interest.  Specifying these parameters can get quite complex; we will limit our analysis for now to working with a minimum number of them:

* number of sequences in the sample (nsam)
* number of simulations to perform (nreps)
* theta (-t, the neutral parameter 4Ne&mu;)
* Number of segregating sites (-s)

Note that at a minimum, nsam, nreps, and *either* -s or -t must be specified.  

The format of the basic call to ms (as implemented in phyclust) is

ms(nsam=k, nreps=n,opts=c("-t ..."))

where k and n are user-specified numbers, and opts is a character vector specifying other paramters to be passed to ms.

##### Basic simulations

#### Fixed value of &theta;

We can start by running a single simulation of the evolution of 10 sequences setting &theta; = 3 (a biologically meaningful number):
```{r}
set.seed(1234)
sim1 <-ms(nsam=10,nreps=1,opts=c("-t 3"))
sim1
```
We see that we get what is essentially character output, containing the following:

1. The call made to ms (written in the format we would use from the command line).
2.  a line containing "//"
3.  The number of segregating sites (10 in this case)
4.  The relative positions of those sites
5.  10 lines containing the simulated sequences, where zeros connote ancestral alleles and 1 indicates derived ones (the result of mutation in the gene geneology)

Note that in this format, the utility of the output for further analysis is limited.  This is where the read.ms.output function can come in - it will convert raw ms output into a list.  However, since this function is embedded in the various analytical progams in the package, we needn't concern ourselves with it at this point.

Finally, since we set &theta; as the only input option, the number of segregating sites among repeated simulations will vary.  This will become important down the road.

#### Fixed value of segregating sites

A second choice (one often used in real data analysis) is to input the number of segregating sites.  This is something that could be determined from real data.  For example, we can look at a data set for the *acp29* locus from *Drosophila melanogaster*.  to do so, we will use the seg.sites(function) from the package **pegas** (a dependency of TeachingPopGen)

```{r}
data(acp29)
n.sites <-length(seg.sites(acp29))
n.seq <-length(acp29)
n.seq;n.sites
```

This tells us that there are a total of 17 sequences and 15 segregating sites; we can use these values in an ms simulation as follows:

```{r}
sim2 <-ms(nsam=17,nreps=1,opts=c("-s 15"))
sim2
```
Again we get similar output.  However, in this case, repeated simulations would always contain 15 segregating sites.

#### Using the -T option

As noted earlier, there are lots of parameters that can be specified as options for ms.  One particularly valuable one is the -T option.  We can repeat our simulation based on the *acp* data accordingly and examine the first three lines of the output:

```{r}
sim3 <-ms(nsam=17,nreps=1,opts=c("-s 15 -T"))
head(sim3,3)
```
The third line is the simulated tree, coded in Newick format.  We can then plot it as follows, using the read.tree function from **ape**:
```{r,fig.height=5}
tr <-read.tree(text=sim3)
plot(tr)
```


Note that the default condition for read.tree is to read from a file; use of the text= parameter specifies reading of an R object.

##### A few simple analyses

#### Unfolded site frequency spectrum

While actual visualization of site frequency spectra is rarely a part of a formal coalescence analysis, it is important to understand what one is and how it forms the underpinning of muxh of coalescence theory (notably the Tajima test). It is possible, using the following code, to display an unfolded site frequency spectrum, however this function is included mainly to be used in other functions (pi, thetaW, etc)

To display the sfs for our acp29-based simulation, we can proceed as folows
```{r}
sim3.ms <-read.ms.output(sim3) #converts output to a list
gametes <-sim3.ms$gametes[[1]] #extract the gametes from that list
ss <-sfs(gametes,pl=TRUE)
ss
```


#### Computing &pi; &theta;w; and the Tajima statistic

Remember that, given a data set, we can estimate &theta; based on either the average pairwise nucleotide differences per sequence (&pi;) or on the number of segregating sites (&theta;w, the Watterson estimator).  These can now be calculated as follows

```{r}
pi.sim <-pi(ss)
th.sim <-theta.W(ss)
taj.sim <-taj(ss)
c(pi=pi.sim,theta=th.sim,D=taj.sim)
```
In fact, the function **ms.sumstats** does all of the above and then some - it takes raw ms output and calculates a number of statistics, as we can see:

```{r}
sim3.ss <-ms.sumstats(sim3)
sim3.ss
```
In addition to the aforementioned statistics, it also returns Fay and Wu's estimator of theta, as well as their H statistic - two measures of theta that are based on the frequency distributions of derived vs. ancestral alleles - something we will return to when we consider natural selection.

But what does this tell us?  By itself, practically nothing.  If we were to repeat the process, we would get different results - after all, this is a random simulation.  But therein lies the power of simulation, and is where we will go next.


#### Summarizing multiple simulations

Up until now, we have been working with one sample at a time.  But remember, the nsam parameter allows us to run multiple simulations, in which case we could obtain summary statistics for all of them, look at their distributions, and compare those distributions to statistics derived from our data of interest.  This is in fact a very straightforward process.  

To illustrate, let's return to our first set of conditions, in which we simulated evolution of 10 sequences assuming that &theta; is 3.  Now, however, let's do 1000 replicates and calculate the summary statistics

```{r}
sim5 <- ms(nsam=10,nreps=1000, opts="-t 3")
sim5.ms <-ms.sumstats(sim5)
head(sim5.ms)
```
We can then look, both numerically and graphically, at whichever of these we wish.  For example, we can make rough plots of the distributions of S and D, highlighting the upper and lower tails:

```{r,fig.show='hold'}
colhist(sim5.ms$S,labs=c("S","Number","Segregating Sites"))
colhist(sim5.ms$Tajima.D,lab=c("D","Number","Tajima's D"))
```

Alternatively, we can look at this numerically:
```{r}
c(mean.S=mean(sim5.ms$S),quantile(sim5.ms$S,c(.025,.975)))
c(mean.D=mean(sim5.ms$Tajima.D,na.rm=TRUE),quantile(sim5.ms$Tajima.D,c(.025,.975),na.rm=TRUE))
```

### Application to data

But where did our value of &theta; come from in the first place?  In real life, that starts out (and often remains) a mystery.  In fact, what we'd really like to do is to estimate &theta; based on observed values of S and &pi;. 

For this, we will use another data set from *Drosophila*, from the *spaghetti sauce* gene.  :
```{r}
data(spaghetti)
nseq <-length(spaghetti)
n.sites <-length(seg.sites(spaghetti))

```
We can also calculate the Tajima statistic (using a function from pegas)
```{r}
ss.D <-tajima.test(spaghetti)
ss.D$D
```
Now, what we want to do is to perform neutral simulation for a sequence with `r nseq` sequences and `r n.sites`. 
```{r}
sim6 <-ms(nsam=nseq,nreps=1000,opts=paste("-s",n.sites,"-r 0 702",sep=" "))
```
Note that since we know the sequence length we've added that with the -r option, assuming also that no recombination occurs (an oversimplification of course)

And repeating what we did above, we will calculate the summary statistics and the histogram of the Tajima statistic, adding to it a blue line showing the observed value of D:

```{r,fig.show='hold'}
sim6.ms <-ms.sumstats(sim6)
colhist(sim6.ms$Tajima.D,labs=c("D","Number","Tajima's Statistic"))
abline(v=ss.D$D, col="blue")
```

And we see that Dobs is significantly different from the expected value of D (`r mean(sim6.ms$Tajima.D,na.rm=TRUE)`), a value that itself is negative.  But what to make of this?  Negative values of D can result from purifying selection; they can also be a result of demographic history.  Making such inferences goes beyond what we can do at this point.

#### Estimating &theta;

What we can do is to begin to think about how we could estimate &theta, based on data like these.  To do so, we will take a Bayesian approach:

1.  Assume as a prior that &theta; is uniformly distributed between zero and 20
2.  Perform 10,000 ms simulations based on that assumption
3.  Obtain a posterior distribution by selecting those simulations for which the number of variable sites is equal to the number observed in the data (`r n.sites`)
4.  Compare the observed value of D with the distribution of D in the posterior.

First, we generate statistics for our prior distribution (note - this code takes a while to run)
```{r}
th <-runif(1000,0,20)
sim7 <-lapply(th,function(x) ms(nsam=nseq,nreps=1,opts=(paste("-t",x,"-r 0 702",sep=" "))))
sim7.sum <-data.frame(t(sapply(sim7,ms.sumstats)))
head(sim7.sum)
```
Now, we select those values of &theta; which resulted in `r n.sites` segregating sites and plot their distribution
```{r}
post.th <-(th[which(sim7.sum$S==n.sites)])
mean.post <-mean(post.th)
colhist(post.th)
abline(v=mean.post,col="blue")
```

The mean of the distribution is `r mean.post`, but note the size of the 95% credible interval - from ~7 to ~19.  And we can also look at the distribution of D from the posterior:
```
```{r,fig.show='hold'}

sim7.post <-(sim7.sum[which(sim7.sum$S==n.sites),])
post.D <-as.numeric(sim7.post$Tajima.D)
colhist(post.D)
c(mean.D=mean(post.D),quantile(post.D,c(.025,.975)))
```
So what does this tell us?  In all likelihood, our sole prior assumption, that &theta; is drawn from U(0,20) is not sufficient as an evolutionary model.  Other factors, such as variation in effective population size, population subdivision, and/or natural selection, have quite likely been contributing factors. Indeed, per the multilocus analysis of [Legrand et al. (2011)](http://onlinelibrary.wiley.com/doi/10.1111/j.1365-294X.2011.05127.x/abstract) these sequences were derived from separate populations tha have undegone minimal historical gene flow.  Addressing these more complex models requires more sophisticated analytical tools (e. g. Markov Chain Monte Carlo methods) that lie beyond the scope of this package.
